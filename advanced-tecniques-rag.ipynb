{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe",
   "metadata": {},
   "source": [
    "# Advanced techniques to improve RAG \n",
    "\n",
    "\n",
    "* video tutorial: https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x\n",
    "* repository: https://github.com/langchain-ai/rag-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218daf25-3d66-4730-a296-bf866cdf2eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46fa56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecemos los modelos que vamos a usar\n",
    "EMBD_MODEL=\"text-embedding-3-small\"\n",
    "LLM_MODEL=\"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1387ced-eeae-4581-8731-f66b5af2de1b",
   "metadata": {},
   "source": [
    "#### Load the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06208df8-54c7-4ed2-b4ee-b28cc999120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f83d28-e100-4d51-ab81-894ee18d4158",
   "metadata": {},
   "source": [
    "#### **Indexing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4472285",
   "metadata": {},
   "source": [
    "##### 1. Load from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9e108f6-92be-452a-9eb0-2af6613c7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document, in this case from web page\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0208a-1bd6-4511-bb27-942a0f85de3d",
   "metadata": {},
   "source": [
    "##### 2. Split the document in small chunks of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "662ef6db-f3f6-46ac-8494-f37481c462fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split document in small chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72035fc9-4b29-4f1f-b885-419c0736dac8",
   "metadata": {},
   "source": [
    "##### Convert the chunks of text in numbers (embeddings) and load them into a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98a80bb7-38a3-47aa-8b6a-0748d1594675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert small chunks in numbers (embeddings) and store in vector database\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OpenAIEmbeddings(model=EMBD_MODEL),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a50ef0-9cdc-4e52-9b47-fc96ac4fa6de",
   "metadata": {},
   "source": [
    "#### **Retrieve and Generate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8a4a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "078e5628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI(model_name=LLM_MODEL, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c12d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a752c6-270a-4e43-ab00-6cce00208380",
   "metadata": {},
   "source": [
    "### Advanced techniques to improve RAG Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f59f5e1-1086-4e72-9fa3-3454f2f055fa",
   "metadata": {},
   "source": [
    "#### 1. Query translation \n",
    "* Improving a question before it might lead to a low quality anwser from our RAG application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa9dd91-d0ac-4b7b-9cd4-db067c8d45c8",
   "metadata": {},
   "source": [
    "#### Alternative ways to transform the initial question:\n",
    "* Convert the question in multiple similar questions\n",
    "    * Multi-Query\n",
    "    * RAG Fusion\n",
    "* Convert the question in (progressive or not) subquestions\n",
    "* Step-back questions (step-back prompting): go one step back and make the questions that precede or originate the user's question. Example:\n",
    "    * User question: where were Mozart born?\n",
    "    * Step-back question: who was Mozart? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f2bcc-a756-4b93-8967-758bfebede4d",
   "metadata": {},
   "source": [
    "## Query translation technique #1: Multi-Query\n",
    "* Convert the question into several similar questions\n",
    "* Get the RAG answer for each of them\n",
    "* Analyze all the answers and produce a final one "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda6dc5-3c60-4525-87d2-509df09cc6ae",
   "metadata": {},
   "source": [
    "#### Create a prompt template to create 5 similar questions to the question provided by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81ba73ef-4026-4580-9d3f-3f2b91d560e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Query: Different Perspectives\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aede4e9-f252-4666-91bc-e2cc6812be19",
   "metadata": {},
   "source": [
    "#### Define a chain to generate the 5 similar questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4c9bb84-a5dd-48ae-8e68-474b491c3418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What does task decomposition mean in the context of LLM agents?  ',\n",
       " 'How do LLM agents utilize task decomposition in their processes?  ',\n",
       " 'Can you explain the concept of task decomposition as it relates to LLM agents?  ',\n",
       " 'In what ways does task decomposition apply to agents powered by LLMs?  ',\n",
       " 'What are the benefits of task decomposition for agents using large language models?  ']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "queries = generate_queries.invoke(question)\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0d933-7c3d-4d3b-a34a-5bb3a4a95dc3",
   "metadata": {},
   "source": [
    "#### Get the RAG answer for each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e307a11-1d7d-446b-9cf1-cd28fed92e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9657c9b-fc7d-41ae-9983-a2822e95ddcb",
   "metadata": {},
   "source": [
    "#### Analyze all the answers and produce a final one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c778a7c0-103e-4588-9f8c-c98a6353f947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. This process allows the agent to transform big tasks into multiple manageable tasks, shedding light on the interpretation of the model's thinking process.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f77bfe9-fc31-4e8e-aa0f-a25ebf234e71",
   "metadata": {},
   "source": [
    "## Query translation technique #2: RAG Fusion\n",
    "* Same as Multi-Query, but ranking the resulting questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c30b9a-860d-4ded-89ff-65155e3b4845",
   "metadata": {},
   "source": [
    "#### Same as before: create a prompt template to create 4 similar questions to the question provided by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ba3565a-935a-416b-affe-4a0fd21afc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a39789-4695-4f49-8450-f1db7f083bec",
   "metadata": {},
   "source": [
    "#### Same as before: define a chain to generate the 4 similar questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d77b333-ad80-432e-88ae-fe402f996dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4de1e7e1-9aef-4535-a448-0ea0fc852468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "# https://medium.com/@nageshmashette32/langchain-rag-fusion-advance-rag-32eefc63da99\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3e704b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (large language model) agents involves breaking down complex tasks into smaller, more manageable steps. This process is enhanced by techniques such as Chain of Thought (CoT), where the model is prompted to \"think step by step,\" allowing it to utilize more computational resources to simplify difficult tasks. By transforming large tasks into multiple smaller tasks, task decomposition not only makes the overall task easier to handle but also provides insight into the model\\'s reasoning process. Additionally, task decomposition can be achieved through various methods, including simple prompting, task-specific instructions, or human inputs.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791c7d7-e751-4d4b-aef0-d381273059a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Query translation technique #3: Decomposition\n",
    "* Decompose de question into progressive sub-questions.\n",
    "https://python.langchain.com/v0.1/docs/use_cases/query_analysis/techniques/decomposition/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e61183-1bb3-4f64-910d-630c45759589",
   "metadata": {},
   "source": [
    "#### Create a prompt template to create 3 subquestions from the question provided by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8acdf02d-6c96-4907-8392-866c22d2b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "\n",
    "Perform query decomposition. Given a user question, break it down into distinct sub questions that \\\n",
    "you need to answer in order to answer the original question.\n",
    "\n",
    "Generate multiple search sub-queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5f4338b9-d95d-4a94-886d-f72e0979de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "question = \"How to Task Decomposition and add Memory\"\n",
    "\n",
    "# These are the subquestions:\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e3121ac-1d15-47a0-b9e5-bf0cd10eda0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What are the different methods of task decomposition?',\n",
       " '2. How does task decomposition help in improving memory retention?',\n",
       " '3. Can you provide examples of task decomposition techniques that aid memory enhancement?']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the subquestions:\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c8fe9-f6c4-46f7-b545-6c4e3fca4bcb",
   "metadata": {},
   "source": [
    "#### Option 1: answer each of the subquestions using the response to the previous subquestion as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "02c3af77-85af-41f5-aa10-0afbb9f5c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f7fab6c-e271-4711-b6bb-35d7bcb0f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "469351b0-e861-4d14-9585-7eebbe2cf8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition techniques that aid memory enhancement involve breaking down complex tasks into smaller, more manageable subgoals. By focusing on specific components, individuals can improve memory retention and recall. Some examples of task decomposition techniques include:\\n\\n1. Decomposing a problem into multiple thought steps and generating multiple thoughts per step, creating a tree structure.\\n2. Using LLM with simple prompting like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"\\n3. Employing task-specific instructions, such as \"Write a story outline\" for writing a novel.\\n4. Involving human inputs in the task decomposition process.\\n\\nThese techniques help individuals encode and store information more effectively in both short-term and long-term memory, ultimately enhancing memory retention and recall abilities.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ebb22b-7a35-47ee-a76d-4c5d9c3c0bb3",
   "metadata": {},
   "source": [
    "#### Option 2: answer each of the subquestions individually (not using the response to the previous subquestion as context).\n",
    "* Useful when the subquestions are not dependent from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9ecd205f-efad-4d65-a9b9-c8779fc1e5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The key steps involved in task decomposition include instructing the model to \"think step by step\" to break down complex tasks into smaller steps. This can be achieved through simple prompting by LLM, task-specific instructions, or human inputs. Task decomposition aims to transform big tasks into multiple manageable tasks for better performance.', \"Task decomposition helps in improving memory retention by breaking down large tasks into smaller, manageable subgoals. This process enables efficient handling of complex tasks and enhances the agent's ability to retain and recall information over extended periods. Memory retention is further improved through the agent's capability to reflect on past actions, learn from mistakes, and refine strategies for future steps.\", 'Memory techniques such as short-term and long-term memory utilization can enhance task decomposition by providing the agent with the ability to retain and recall information over time. These memory capabilities can help in improving the quality of final results by learning from past actions and mistakes. Specific memory techniques can be beneficial when used in conjunction with task decomposition for better outcomes.']\n",
      "=====\n",
      "['1. What are the key steps involved in task decomposition?', '2. How does task decomposition help in improving memory retention?', '3. Are there specific memory techniques that can be used in conjunction with task decomposition for better results?']\n"
     ]
    }
   ],
   "source": [
    "# Answer each sub-question individually\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# RAG prompt\n",
    "prompt_rag_template = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt_rag = ChatPromptTemplate.from_template(prompt_rag_template)\n",
    "\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "\n",
    "    # Use our decomposition /\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\": question})\n",
    "\n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "\n",
    "    for sub_question in sub_questions:\n",
    "\n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "\n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke(\n",
    "            {\"context\": retrieved_docs, \"question\": sub_question}\n",
    "        )\n",
    "        rag_results.append(answer)\n",
    "\n",
    "    return rag_results, sub_questions\n",
    "\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(\n",
    "    question, prompt_rag, generate_queries_decomposition\n",
    ")\n",
    "print(answers)\n",
    "print('=====')\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9a7847d2-b729-4435-92e7-d80fe014f435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition involves breaking down complex tasks into smaller, manageable subgoals to improve performance. By instructing the model to think step by step, it can effectively handle large tasks and enhance memory retention. Memory techniques such as short-term and long-term memory utilization can be used in conjunction with task decomposition to further enhance memory retention and improve the quality of final results. By combining task decomposition with memory techniques, agents can efficiently handle complex tasks, retain and recall information over time, learn from past actions, and refine strategies for better outcomes.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":context,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f64ba7-3a7f-4eac-b429-985c200ce2cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Query translation technique #4: Step-back questions (also called step-back prompting)\n",
    "* Step-back questions (step-back prompting): go one step back and make the questions that precede or originate the user's question. Example:\n",
    "    * User question: where were Mozart born?\n",
    "    * Step-back question: who was Mozart? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac6c2464-04e7-49ec-a26f-e00f36be3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0d8d4f28-4da4-4fb1-8e21-9f3d34c940a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the process of breaking down tasks for LLM agents?'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | llm | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1281ec03-54b4-4bf6-8d05-c955241799b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down complex tasks into smaller and more manageable subtasks. This process allows the LLM (large language model) to better understand the overall task and approach it in a step-by-step manner. By decomposing tasks, the LLM can effectively plan and execute each subtask, leading to more efficient problem-solving and decision-making.\\n\\nOne common technique used for task decomposition is the Chain of Thought (CoT), where the model is prompted to \"think step by step\" to decompose hard tasks into simpler steps. This approach helps the LLM transform large and complex tasks into a series of smaller tasks, making it easier to tackle each component individually.\\n\\nAdditionally, the Tree of Thoughts extends the concept of CoT by exploring multiple reasoning possibilities at each step. It generates multiple thoughts per step, creating a tree structure of task decomposition. This method allows for a more comprehensive analysis of the problem and helps the LLM consider various paths to reach the solution.\\n\\nTask decomposition can be facilitated through simple prompting, task-specific instructions, or human inputs, depending on the nature of the task at hand. Overall, task decomposition plays a crucial role in enhancing the performance of LLM-powered autonomous agents by breaking down complex tasks into manageable steps for efficient problem-solving.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Response prompt \n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627eef9a-c857-4bcc-b19f-ffe1c469b471",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Query Translation Technique #5: The HyDE Technique\n",
    "* Given the user's question, create a fake document that would answer it properly.\n",
    "* Then go to the vector database and find documents similar to the fake document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e15803a2-5e3f-4932-a54b-212d05469699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a fundamental concept in the field of reinforcement learning and artificial intelligence, particularly for Large Language Models (LLMs) agents. Task decomposition refers to the process of breaking down a complex task into smaller, more manageable sub-tasks or components. This allows the agent to effectively tackle the overall task by focusing on individual components sequentially or in parallel.\\n\\nIn the context of LLM agents, task decomposition is crucial for improving the efficiency and effectiveness of the learning process. By breaking down a complex task into smaller sub-tasks, LLM agents can better understand the underlying structure and dependencies within the task. This enables the agent to learn more efficiently and make better decisions when faced with complex and ambiguous situations.\\n\\nFurthermore, task decomposition can also help LLM agents to generalize their learning across different tasks and domains. By decomposing tasks into smaller components, LLM agents can identify common patterns and strategies that can be applied to a wide range of tasks. This allows the agent to transfer its knowledge and skills to new tasks more effectively, ultimately improving its overall performance and adaptability.\\n\\nOverall, task decomposition plays a critical role in the development and optimization of LLM agents, enabling them to tackle complex tasks more effectively and efficiently. By breaking down tasks into smaller components, LLM agents can improve their learning process, generalize their knowledge, and enhance their overall performance in a variety of tasks and domains.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HyDE document generation\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c0501fa6-6b20-406d-aee4-db346295ac05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'),\n",
       " Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'),\n",
       " Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retireved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "retireved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "546e95d4-2276-447f-a6ea-53e9f35f5e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down complex tasks into smaller and simpler steps. This process allows the agent to better understand the task at hand and plan ahead effectively. Task decomposition can be achieved through techniques like Chain of Thought (CoT) and Tree of Thoughts, which help the LLM agent explore multiple reasoning possibilities at each step and generate multiple thoughts per step, creating a structured approach to problem-solving. Task decomposition can be facilitated by simple prompting, task-specific instructions, or human inputs.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f8b6f1-462c-4e42-ad42-eebf6fb1e3c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Routing\n",
    "Purpose: To direct the question to the appropriate data source (such as a vector database, relational database, or graph database) or the appropriate prompt or any other routing option.\n",
    "\n",
    "Types of Routing:\n",
    "* Logical Routing: Uses the LLM's knowledge of data sources to decide the best destination for a query.\n",
    "* Semantic Routing: Involves embedding the question and prompts, calculating similarity, and selecting the prompt with the highest similarity for routing. For example: based on the question, use the Math prompt or the Physics prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb3a86-8b1d-46ad-964d-bf48768cf3f5",
   "metadata": {},
   "source": [
    "#### Logical Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ac3aa8f3-7d35-40bf-8bd1-fae570e133aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gaarango\\Projectos\\Indra\\CAPACITACIONES\\rag-applications-intro\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "99e24506-9f17-4983-93be-fb551d8e98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5b1cb1c8-d973-441a-b5fa-577c31f7604c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='python_docs')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0a691fef-02b1-4593-b30a-78930801f766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python_docs'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8aed77ca-cc8b-4853-b95a-9e3d22cad237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for python_docs\"\n",
    "    elif \"js_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for js_docs\"\n",
    "    else:\n",
    "        ### Logic here \n",
    "        return \"golang_docs\"\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8779d80d-2f31-4103-9019-d0740a2769d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chain for python_docs'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ea7aa9-8652-4e20-853f-90a9fcbd9037",
   "metadata": {},
   "source": [
    "#### Semantic Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f4e5160f-efc6-4bbd-a3b5-7447481c4e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n",
      "A black hole is a region in space where the gravitational pull is so strong that nothing, not even light, can escape from it. It is formed when a massive star collapses under its own gravity. The boundary around a black hole is called the event horizon, beyond which nothing can escape. Black holes can vary in size and mass, with supermassive black holes being found at the center of galaxies.\n"
     ]
    }
   ],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Two prompts\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "# Embed prompts\n",
    "embeddings = OpenAIEmbeddings()\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "# Route question to prompt \n",
    "def prompt_router(input):\n",
    "    # Embed question\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    # Compute similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    # Chosen prompt \n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What's a black hole\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9d4ef-efce-4885-9f54-262fb8e5afdd",
   "metadata": {},
   "source": [
    "# Query structuring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67554be7-8609-4a2e-af6e-7c4f68a7774e",
   "metadata": {},
   "source": [
    "Transform the query from natural language to some query syntax.\n",
    "\n",
    "In the example below, we transform the query from natural language to the query syntax according to the schema we have defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd7f4b9-be4e-4b9f-b05c-c2fa97b2f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "docs = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=pbAd8O1Lvm4\", add_video_info=True\n",
    ").load()\n",
    "\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a21dbe-a1d6-4ea3-b90c-f0a58098aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Literal, Optional, Tuple\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class TutorialSearch(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    content_search: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    title_search: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Alternate version of the content search query to apply to video titles. \"\n",
    "            \"Should be succinct and only include key words that could be in a video \"\n",
    "            \"title.\"\n",
    "        ),\n",
    "    )\n",
    "    min_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum view count filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum view count filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    earliest_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Earliest publish date filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    latest_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Latest publish date filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    min_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum video length in seconds, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum video length in seconds, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        for field in self.__fields__:\n",
    "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
    "                self.__fields__[field], \"default\", None\n",
    "            ):\n",
    "                print(f\"{field}: {getattr(self, field)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b3cb7a-52f0-48c5-9774-b70cdf5ebe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a database query optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(TutorialSearch)\n",
    "query_analyzer = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07610b1-560f-411d-ad7b-628ff7850344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e6941-99ad-4ebe-afe5-aa14c8832b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_analyzer.invoke({\"question\": \"rag from scratch\"}).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff61e88-91af-43e2-a1a4-5d2730e33b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_analyzer.invoke(\n",
    "    {\"question\": \"videos on chat langchain published in 2023\"}\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657a40cf-004a-4470-9da7-3199f7d11b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_analyzer.invoke(\n",
    "    {\n",
    "        \"question\": \"how to use multi-modal models in an agent, only videos under 5 minutes\"\n",
    "    }\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74344f8-483f-4d71-8f0f-05194d132416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
