{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q langchain langchain-community langchain-openai langchain-chroma python-dotenv pypdf bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecemos los modelos que vamos a usar\n",
    "EMBD_MODEL=\"text-embedding-3-small\"\n",
    "LLM_MODEL=\"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "document_path = \"resources/00 SCRUMstudy-SBOK-Guide-3rd-edition-Spanish.pdf\"\n",
    "\n",
    "# loader = PyPDFLoader(file_path=document_path)\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Split document loaded into a list of documents List[Document]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2048)\n",
    "docs = text_splitter.split_documents(document)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Embeddings and Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "emdb = OpenAIEmbeddings(model=EMBD_MODEL)\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=emdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve and Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "question = 'What are the approaches to Task Decomposition?'\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {input} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "qa_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ],\n",
    "    input_variables=[\"context\", \"input\"]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=LLM_MODEL)\n",
    "\n",
    "qa_chain = {\n",
    "    \"context\": retriever,\n",
    "    \"input\": RunnablePassthrough()\n",
    "} | qa_prompt | llm | StrOutputParser()\n",
    "\n",
    "# question = 'En que consiste el enfoque Task Decomposition?'\n",
    "\n",
    "response = qa_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
