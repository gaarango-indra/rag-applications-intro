{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q llama-index llama-index-vector-stores-chroma chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecemos los modelos que vamos a usar\n",
    "EMBD_MODEL=\"text-embedding-3-small\"\n",
    "LLM_MODEL=\"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Load Documents from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "documents = SimpleDirectoryReader(\"resources\").load_data()\n",
    "\n",
    "# initialize client, setting path to save data\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# create collection\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    storage_context=storage_context\n",
    ")\n",
    "\n",
    "# create a query engine and query\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the document about?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Load Document from Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q llama-index-readers-web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "urls = [\"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\n",
    "documents = SimpleWebPageReader().load_data(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.text_splitter import TokenTextSplitter\n",
    "from llama_index.core import Document\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "    \n",
    "split_documents: list[str] = []\n",
    "for doc in documents:\n",
    "    chunks = text_splitter.split_text(doc.text)\n",
    "    split_documents.extend(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Embeddings and Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model=EMBD_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='b8920c20-5383-4691-8981-711fbe1baf88', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7e4a2087-327e-4c12-8bc9-122bf48a8dcb', node_type='4', metadata={}, hash='c68a1eaade2e8496c2a5aea772e5e2bfa7986a1d9dc4cb3f636098e9a615d938')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='<a href=\"#challenges\" aria-label=\"Challenges\">Challenges</a></li>\\n                <li>\\n                    <a href=\"#citation\" aria-label=\"Citation\">Citation</a></li>\\n                <li>\\n                    <a href=\"#references\" aria-label=\"References\">References</a>\\n                </li>\\n            </ul>\\n        </div>\\n    </details>\\n</div>\\n\\n  <div class=\"post-content\"><p>Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as <a href=\"https://github.com/Significant-Gravitas/Auto-GPT\">AutoGPT</a>, <a href=\"https://github.com/AntonOsika/gpt-engineer\">GPT-Engineer</a> and <a href=\"https://github.com/yoheinakajima/babyagi\">BabyAGI</a>, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.</p>\\n<h1 id=\"agent-system-overview\">Agent System Overview<a hidden class=\"anchor\" aria-hidden=\"true\" href=\"#agent-system-overview\">#</a></h1>\\n<p>In a LLM-powered autonomous agent system, LLM functions as the agent&rsquo;s brain, complemented by several key components:</p>\\n<ul>\\n<li><strong>Planning</strong>\\n<ul>\\n<li>Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.</li>\\n<li>Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.</li>\\n</ul>\\n</li>\\n<li><strong>Memory</strong>\\n<ul>\\n<li>Short-term memory: I would consider all the in-context learning (See <a href=\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\">Prompt Engineering</a>) as utilizing short-term memory of the model to learn.</li>\\n<li>Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.</li>\\n</ul>\\n</li>\\n<li><strong>Tool use</strong>\\n<ul>\\n<li>The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.</li>\\n</ul>\\n</li>\\n</ul>\\n<img src=\"agent-overview.png\" style=\"width: 100%;\" class=\"center\" />\\n<figcaption>Fig. 1. Overview of a LLM-powered autonomous agent system.</figcaption>\\n<h1 id=\"component-one-planning\">Component One: Planning<a hidden class=\"anchor\" aria-hidden=\"true\" href=\"#component-one-planning\">#</a></h1>\\n<p>A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.</p>\\n<h2 id=\"task-decomposition\">Task Decomposition<a hidden class=\"anchor\" aria-hidden=\"true\" href=\"#task-decomposition\">#</a></h2>\\n<p><a href=\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#chain-of-thought-cot\"><strong>Chain of thought</strong></a> (CoT; <a href=\"https://arxiv.org/abs/2201.11903\">Wei et al. 2022</a>) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to &ldquo;think step by step&rdquo; to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model&rsquo;s thinking process.</p>\\n<p><strong>Tree of Thoughts</strong> (<a href=\"https://arxiv.org/abs/2305.10601\">Yao et al. 2023</a>) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the', mimetype='text/plain', start_char_idx=0, end_char_idx=3702, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.40947507078479434),\n",
       " NodeWithScore(node=TextNode(id_='6aeb4be9-b2c7-43f8-bd9c-6f4b8cacc4f0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1c19a467-a504-4dc7-941f-c6659352976d', node_type='4', metadata={}, hash='450dddcdc858b9b85ba694b214009e0d7faa921a43c184ca37dad5422eec2db0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='<a href=\"#component-one-planning\" aria-label=\"Component One: Planning\">Component One: Planning</a><ul>\\n                        \\n                <li>\\n                    <a href=\"#task-decomposition\" aria-label=\"Task Decomposition\">Task Decomposition</a></li>\\n                <li>\\n                    <a href=\"#self-reflection\" aria-label=\"Self-Reflection\">Self-Reflection</a></li></ul>\\n                </li>\\n                <li>\\n                    <a href=\"#component-two-memory\" aria-label=\"Component Two: Memory\">Component Two: Memory</a><ul>\\n                        \\n                <li>\\n                    <a href=\"#types-of-memory\" aria-label=\"Types of Memory\">Types of Memory</a></li>\\n                <li>\\n                    <a href=\"#maximum-inner-product-search-mips\" aria-label=\"Maximum Inner Product Search (MIPS)\">Maximum Inner Product Search (MIPS)</a></li></ul>\\n                </li>\\n                <li>\\n                    <a href=\"#component-three-tool-use\" aria-label=\"Component Three: Tool Use\">Component Three: Tool Use</a></li>\\n                <li>\\n                    <a href=\"#case-studies\" aria-label=\"Case Studies\">Case Studies</a><ul>\\n                        \\n                <li>\\n                    <a href=\"#scientific-discovery-agent\" aria-label=\"Scientific Discovery Agent\">Scientific Discovery Agent</a></li>\\n                <li>\\n                    <a href=\"#generative-agents-simulation\" aria-label=\"Generative Agents Simulation\">Generative Agents Simulation</a></li>\\n                <li>\\n                    <a href=\"#proof-of-concept-examples\" aria-label=\"Proof-of-Concept Examples\">Proof-of-Concept Examples</a></li></ul>\\n                </li>\\n                <li>\\n                    <a href=\"#challenges\" aria-label=\"Challenges\">Challenges</a></li>\\n                <li>\\n                    <a href=\"#citation\" aria-label=\"Citation\">Citation</a></li>\\n                <li>\\n                    <a href=\"#references\" aria-label=\"References\">References</a>\\n                </li>\\n            </ul>\\n        </div>\\n    </details>\\n</div>\\n\\n  <div', mimetype='text/plain', start_char_idx=0, end_char_idx=2077, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.39875511499479566)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import GPTVectorStoreIndex\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents(\n",
    "    documents=[Document(text=text) for text in split_documents],\n",
    ")\n",
    "question = 'What are the approaches to Task Decomposition?'\n",
    "retriever = index.as_retriever()\n",
    "\n",
    "# Returns Node with score\n",
    "nodes = retriever.retrieve(question)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "QUERY_PROMPT = \"\"\"\n",
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {input} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "Settings.llm = OpenAI(model=LLM_MODEL)\n",
    "\n",
    "query_engine = index.as_chat_engine(prompt=QUERY_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The approaches to Task Decomposition include:\\n\\n1. **Chain of Thought (CoT)**: This technique enhances model performance by instructing the model to \"think step by step.\" It breaks down complex tasks into smaller, manageable steps, allowing for clearer reasoning and problem-solving.\\n\\n2. **Tree of Thoughts**: This approach extends the Chain of Thought concept by exploring multiple reasoning possibilities at each step. It allows for a more comprehensive decomposition of tasks by considering various paths and outcomes, facilitating a deeper understanding of the task at hand.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question = 'En que consiste el enfoque Task Decomposition?'\n",
    "# question = 'QuiÃ©n es el mejor jugador de futbol segun la web?'\n",
    "\n",
    "answer = query_engine.query(question)\n",
    "answer.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Advanced query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Task Decomposition can be approached using techniques such as Chain of Thought (CoT) and Tree of Thoughts. CoT involves instructing the model to \"think step by step,\" allowing it to break down complex tasks into smaller, manageable steps. This method enhances the model\\'s performance on difficult tasks by transforming them into simpler components. Tree of Thoughts extends this concept by exploring multiple reasoning possibilities at each step, further aiding in the decomposition of tasks.', source_nodes=[NodeWithScore(node=TextNode(id_='b8920c20-5383-4691-8981-711fbe1baf88', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7e4a2087-327e-4c12-8bc9-122bf48a8dcb', node_type='4', metadata={}, hash='c68a1eaade2e8496c2a5aea772e5e2bfa7986a1d9dc4cb3f636098e9a615d938')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='<a href=\"#challenges\" aria-label=\"Challenges\">Challenges</a></li>\\n                <li>\\n                    <a href=\"#citation\" aria-label=\"Citation\">Citation</a></li>\\n                <li>\\n                    <a href=\"#references\" aria-label=\"References\">References</a>\\n                </li>\\n            </ul>\\n        </div>\\n    </details>\\n</div>\\n\\n  <div class=\"post-content\"><p>Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as <a href=\"https://github.com/Significant-Gravitas/Auto-GPT\">AutoGPT</a>, <a href=\"https://github.com/AntonOsika/gpt-engineer\">GPT-Engineer</a> and <a href=\"https://github.com/yoheinakajima/babyagi\">BabyAGI</a>, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.</p>\\n<h1 id=\"agent-system-overview\">Agent System Overview<a hidden class=\"anchor\" aria-hidden=\"true\" href=\"#agent-system-overview\">#</a></h1>\\n<p>In a LLM-powered autonomous agent system, LLM functions as the agent&rsquo;s brain, complemented by several key components:</p>\\n<ul>\\n<li><strong>Planning</strong>\\n<ul>\\n<li>Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.</li>\\n<li>Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.</li>\\n</ul>\\n</li>\\n<li><strong>Memory</strong>\\n<ul>\\n<li>Short-term memory: I would consider all the in-context learning (See <a href=\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\">Prompt Engineering</a>) as utilizing short-term memory of the model to learn.</li>\\n<li>Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.</li>\\n</ul>\\n</li>\\n<li><strong>Tool use</strong>\\n<ul>\\n<li>The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.</li>\\n</ul>\\n</li>\\n</ul>\\n<img src=\"agent-overview.png\" style=\"width: 100%;\" class=\"center\" />\\n<figcaption>Fig. 1. Overview of a LLM-powered autonomous agent system.</figcaption>\\n<h1 id=\"component-one-planning\">Component One: Planning<a hidden class=\"anchor\" aria-hidden=\"true\" href=\"#component-one-planning\">#</a></h1>\\n<p>A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.</p>\\n<h2 id=\"task-decomposition\">Task Decomposition<a hidden class=\"anchor\" aria-hidden=\"true\" href=\"#task-decomposition\">#</a></h2>\\n<p><a href=\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#chain-of-thought-cot\"><strong>Chain of thought</strong></a> (CoT; <a href=\"https://arxiv.org/abs/2201.11903\">Wei et al. 2022</a>) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to &ldquo;think step by step&rdquo; to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model&rsquo;s thinking process.</p>\\n<p><strong>Tree of Thoughts</strong> (<a href=\"https://arxiv.org/abs/2305.10601\">Yao et al. 2023</a>) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the', mimetype='text/plain', start_char_idx=0, end_char_idx=3702, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.40947507078479434), NodeWithScore(node=TextNode(id_='6aeb4be9-b2c7-43f8-bd9c-6f4b8cacc4f0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1c19a467-a504-4dc7-941f-c6659352976d', node_type='4', metadata={}, hash='450dddcdc858b9b85ba694b214009e0d7faa921a43c184ca37dad5422eec2db0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='<a href=\"#component-one-planning\" aria-label=\"Component One: Planning\">Component One: Planning</a><ul>\\n                        \\n                <li>\\n                    <a href=\"#task-decomposition\" aria-label=\"Task Decomposition\">Task Decomposition</a></li>\\n                <li>\\n                    <a href=\"#self-reflection\" aria-label=\"Self-Reflection\">Self-Reflection</a></li></ul>\\n                </li>\\n                <li>\\n                    <a href=\"#component-two-memory\" aria-label=\"Component Two: Memory\">Component Two: Memory</a><ul>\\n                        \\n                <li>\\n                    <a href=\"#types-of-memory\" aria-label=\"Types of Memory\">Types of Memory</a></li>\\n                <li>\\n                    <a href=\"#maximum-inner-product-search-mips\" aria-label=\"Maximum Inner Product Search (MIPS)\">Maximum Inner Product Search (MIPS)</a></li></ul>\\n                </li>\\n                <li>\\n                    <a href=\"#component-three-tool-use\" aria-label=\"Component Three: Tool Use\">Component Three: Tool Use</a></li>\\n                <li>\\n                    <a href=\"#case-studies\" aria-label=\"Case Studies\">Case Studies</a><ul>\\n                        \\n                <li>\\n                    <a href=\"#scientific-discovery-agent\" aria-label=\"Scientific Discovery Agent\">Scientific Discovery Agent</a></li>\\n                <li>\\n                    <a href=\"#generative-agents-simulation\" aria-label=\"Generative Agents Simulation\">Generative Agents Simulation</a></li>\\n                <li>\\n                    <a href=\"#proof-of-concept-examples\" aria-label=\"Proof-of-Concept Examples\">Proof-of-Concept Examples</a></li></ul>\\n                </li>\\n                <li>\\n                    <a href=\"#challenges\" aria-label=\"Challenges\">Challenges</a></li>\\n                <li>\\n                    <a href=\"#citation\" aria-label=\"Citation\">Citation</a></li>\\n                <li>\\n                    <a href=\"#references\" aria-label=\"References\">References</a>\\n                </li>\\n            </ul>\\n        </div>\\n    </details>\\n</div>\\n\\n  <div', mimetype='text/plain', start_char_idx=0, end_char_idx=2077, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.39875511499479566)], metadata={'b8920c20-5383-4691-8981-711fbe1baf88': {}, '6aeb4be9-b2c7-43f8-bd9c-6f4b8cacc4f0': {}})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# similarity postprocessor: filter nodes below 0.75 similarity score\n",
    "processor = SimilarityPostprocessor(similarity_cutoff=0.75)\n",
    "filtered_nodes = processor.postprocess_nodes(nodes)\n",
    "\n",
    "response_syntetizer = get_response_synthesizer()\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    response_syntetizer=response_syntetizer\n",
    ")\n",
    "query_engine.query(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
